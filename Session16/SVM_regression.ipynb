{
 "cells": [
  {
   "attachments": {
    "svm.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines are perhaps one of the most popular and talked about machine learning\n",
    "algorithms. They were extremely popular around the time they were developed in the 1990s\n",
    "and continue to be the go-to method for a high-performing algorithm with little tuning.\n",
    "\n",
    "The Maximal-Margin Classifier is a hypothetical classifier that best explains how SVM works in\n",
    "practice. The numeric input variables (x) in your data (the columns) form an n-dimensional\n",
    "space. For example, if you had two input variables, this would form a two-dimensional space. A\n",
    "hyperplane is a line that splits the input variable space. In SVM, a hyperplane is selected to\n",
    "best separate the points in the input variable space by their class, either class 0 or class 1. In\n",
    "two-dimensions you can visualize this as a line and let’s assume that all of our input points can\n",
    "be completely separated by this line. For example:\n",
    "\n",
    "B0 + (B1 × X1) + (B2 × X2) = 0\n",
    "\n",
    "Where the coefficients (B1 and B2) that determine the slope of the line and the intercept\n",
    "(B0) are found by the learning algorithm, and X1 and X2 are the two input variables.You can\n",
    "make classifications using this line. By plugging in input values into the line equation, you can\n",
    "calculate whether a new point is above or below the line.\n",
    "\n",
    "* Above the line, the equation returns a value greater than 0 and the point belongs to the first class (class 0).\n",
    "* Below the line, the equation returns a value less than 0 and the point belongs to the second class (class 1).\n",
    "* A value close to the line returns a value close to zero and the point may be difficult to classify.\n",
    "* If the magnitude of the value is large, the model may have more confidence in the prediction.\n",
    "\n",
    "The distance between the line and the closest data points is referred to as the margin. The\n",
    "best or optimal line that can separate the two classes is the line that as the largest margin.\n",
    "This is called the Maximal-Margin hyperplane. The margin is calculated as the perpendicular\n",
    "distance from the line to only the closest points. Only these points are relevant in defining\n",
    "the line and in the construction of the classifier. These points are called the support vectors.\n",
    "They support or define the hyperplane. The hyperplane is learned from training data using an\n",
    "optimization procedure that maximizes the margin.\n",
    "\n",
    "\n",
    "In practice, real data is messy and cannot be separated perfectly with a hyperplane. The\n",
    "constraint of maximizing the margin of the line that separates the classes must be relaxed. This\n",
    "is often called the soft margin classifier. This change allows some points in the training data to\n",
    "violate the separating line. An additional set of coefficients are introduced that give the margin\n",
    "wiggle room in each dimension. These coefficients are sometimes called slack variables. This\n",
    "increases the complexity of the model as there are more parameters for the model to fit to the\n",
    "data to provide this complexity.\n",
    "A tuning parameter is introduced called simply C that defines the magnitude of the wiggle\n",
    "allowed across all dimensions. The C parameters defines the amount of violation of the margin\n",
    "allowed. A C = 0 is no violation and we are back to the inflexible Maximal-Margin Classifier\n",
    "described above. The larger the value of C the more violations of the hyperplane are permitted.\n",
    "During the learning of the hyperplane from data, all training instances that lie within the\n",
    "distance of the margin will affect the placement of the hyperplane and are referred to as support\n",
    "vectors. And as C affects the number of instances that are allowed to fall within the margin, C\n",
    "influences the number of support vectors used by the model.\n",
    "* The smaller the value of C, the more sensitive the algorithm is to the training data (higher variance and lower bias).\n",
    "* The larger the value of C, the less sensitive the algorithm is to the training data (lower variance and higher bias).\n",
    "\n",
    "The SVM algorithm is implemented in practice using a kernel. The learning of the hyperplane in\n",
    "linear SVM is done by transforming the problem using some linear algebra. A powerful insight is that the linear SVM can be rephrased\n",
    "using the inner product of any two given observations, rather than the observations themselves.\n",
    "\n",
    "The inner product between two vectors is the sum of the multiplication of each pair of input\n",
    "values. For example, the inner product of the vectors [2, 3] and [5, 6] is 2 × 5 + 3 × 6 or 28. The\n",
    "equation for making a prediction for a new input using the dot product between the input (x)\n",
    "and each support vector (x i ) is calculated as follows:\n",
    "\n",
    "![svm.png](attachment:svm.png)\n",
    "\n",
    "This is an equation that involves calculating the inner products of a new input vector (x)\n",
    "with all support vectors in training data. The coefficients B0 and a i (for each input) must be\n",
    "estimated from the training data by the learning algorithm.\n",
    "\n",
    "The SVM model needs to be solved using an optimization procedure. You can use a numerical\n",
    "optimization procedure to search for the coefficients of the hyperplane. This is inefficient and\n",
    "is not the approach used in widely used SVM implementations like LIBSVM. If implementing\n",
    "the algorithm as an exercise, you could use a variation of gradient descent called sub-gradient\n",
    "descent.\n",
    "There are specialized optimization procedures that re-formulate the optimization problem\n",
    "to be a Quadratic Programming problem. The most popular method for fitting SVM is the\n",
    "Sequential Minimal Optimization (SMO) method that is very efficient. It breaks the problem\n",
    "down into sub-problems that can be solved analytically (by calculating) rather than numerically\n",
    "(by searching or optimizing).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Kernel: The function used to map a lower dimensional data into a higher dimensional data.\n",
    "\n",
    "Hyper Plane: In SVM this is basically the separation line between the data classes. Although in SVR we are going to define it as the line that will will help us predict the continuous value or target value\n",
    "\n",
    "Boundary line: In SVM there are two lines other than Hyper Plane which creates a margin . The support vectors can be on the Boundary lines or outside it. This boundary line separates the two classes. In SVR the concept is same.\n",
    "\n",
    "Support vectors: This are the data points which are closest to the boundary. The distance of the points is minimum or least.\n",
    "    \n",
    "In simple regression we try to minimise the error rate. While in SVR we try to fit the error within a certain threshold.\n",
    "\n",
    "Our best fit line is the line hyperplane that has maximum number of points.\n",
    "\n",
    "What we are trying to do here is basically trying to decide a decision boundary at ‘e’ distance from the original hyper plane such that data points closest to the hyper plane or the support vectors are within that boundary line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Position_Salaries.csv')\n",
    "X = dataset.iloc[:, 1:2].values\n",
    "y = dataset.iloc[:, 2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Position  Level   Salary\n",
      "0   Business Analyst      1    45000\n",
      "1  Junior Consultant      2    50000\n",
      "2  Senior Consultant      3    60000\n",
      "3            Manager      4    80000\n",
      "4    Country Manager      5   110000\n",
      "5     Region Manager      6   150000\n",
      "6            Partner      7   200000\n",
      "7     Senior Partner      8   300000\n",
      "8            C-level      9   500000\n",
      "9                CEO     10  1000000\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
       "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting SVR to the dataset\n",
    "from sklearn.svm import SVR\n",
    "regressor = SVR(kernel = 'rbf')\n",
    "#rbf = Gaussian Radial Basis Function Kernel\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]\n",
      " [9]]\n"
     ]
    }
   ],
   "source": [
    "# Predicting a new result\n",
    "print(X_test)\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([129997.29257314, 130002.2779053 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 60000, 500000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
